# Perception Service - Carter System Component

This module provides comprehensive robot perception including SLAM, 3D reconstruction, and navigation for the Carter autonomous system using NVIDIA Isaac ROS Nvblox, VSLAM, and Nav2.

## Overview

The Perception Service is the component in the Carter workflow that:
- Takes depth maps, stereo vision images, and goal poses as input
- Uses stereo vision images with nvslam for robot pose estimation (localization)
- Uses depth maps and robot pose with nvblox for 3D reconstruction and mapping
- Uses costmaps generated by nvblox and goal poses with nav2 for autonomous robot navigation
- Performs real-time SLAM using Visual-Inertial Odometry (VIO)
- Provides complete autonomous navigation capabilities for object following

## Key Features

- **NVSLAM Integration**: Visual-Inertial SLAM using stereo images for accurate robot pose estimation
- **Nvblox 3D Reconstruction**: Volumetric mapping using depth and pose for obstacle avoidance
- **Nav2 Navigation**: Full ROS2 navigation stack using costmaps and goal poses for path planning and execution
- **Dynamic Obstacles**: Handles moving objects and people in the environment
- **Multi-Camera Support**: Configurable for multiple camera setups
- **People Segmentation**: Specialized handling of dynamic human obstacles
- **Complete Pipeline**: Integrates all three components (nvslam, nvblox, nav2) for autonomous navigation

## Prerequisites

### Hardware Requirements
- NVIDIA GPU with CUDA support
- Minimum 8GB GPU VRAM (12GB+ recommended)
- x86_64 or ARM64 architecture
- Sufficient CPU for real-time processing

### Software Requirements
- NVIDIA Isaac ROS development environment
- Docker with NVIDIA runtime support
- CUDA 12.6+ or Jetpack 6.1/6.2
- ROS2 Humble or later

## Installation & Setup

### 1. Setup Isaac ROS Development Environment

Follow the official Isaac ROS setup guide:
```bash
# Set up development environment
# Follow: https://nvidia-isaac-ros.github.io/getting_started/dev_env_setup.html
```

### 2. Build Base Isaac ROS Image

Build the Isaac ROS base image following the ESS documentation:
```bash
# Follow the setup guide here:
# https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_dnn_stereo_depth/isaac_ros_ess/index.html
# Stop after running the container using run_dev script
```

### 3. Build Perception Service Docker Image

**Important: Before building, verify that the base image name in the Dockerfile matches your Isaac ROS setup.**

```bash
# Navigate to Perception directory
cd /path/to/perception

# Verify base image name in Dockerfile
cat Dockerfile | grep FROM

# Build the Docker image
docker build --network=host -t nvblox_ros:x86 .

# For ARM64/Jetson systems
docker build --network=host -t nvblox_ros:arm64 .
```

## Usage

### Standalone Operation

```bash
# Run perception service
docker run -it --rm --privileged --network=host --ipc=host \
  -v /tmp/.X11-unix:/tmp/.X11-unix \
  -v $HOME/.Xauthority:/home/admin/.Xauthority:rw \
  -e DISPLAY -e NVIDIA_VISIBLE_DEVICES=all -e NVIDIA_DRIVER_CAPABILITIES=all \
  -e ROS_DOMAIN_ID -e USER \
  -e HOST_USER_UID=`id -u` -e HOST_USER_GID=`id -g` \
  -v /etc/localtime:/etc/localtime:ro \
  --gpus all nvblox_ros:x86
```

### Integration with Carter System

The Perception Service is automatically managed by the Carter system:

```bash
# Start perception service as part of complete Carter system
./start-carter.sh

# Start only perception service
./start-carter.sh --profile robot-path-planning

# Or using docker-compose directly
docker compose --env-file carter.env --profile robot-path-planning up
```

## ROS2 Topics

### Subscribed Topics (Sensor Input)
- `/front_stereo_camera/left/camera_info` (sensor_msgs/CameraInfo) - Left camera calibration
- `/front_stereo_camera/left/image_rect_color` (sensor_msgs/Image) - Left stereo camera image
- `/front_stereo_camera/right/camera_info` (sensor_msgs/CameraInfo) - Right camera calibration
- `/front_stereo_camera/right/image_rect_color` (sensor_msgs/Image) - Right stereo camera image
- `/front_stereo_imu/imu` (sensor_msgs/Imu) - IMU data for visual-inertial odometry
- `/depth` (sensor_msgs/Image) - Depth information from ESS

### Subscribed Topics (Navigation Input)
- `/goal_pose` (geometry_msgs/PoseStamped) - Navigation goal

### Published Topics (Navigation Output)
- `/cmd_vel` (geometry_msgs/Twist) - Robot velocity commands
- `/nvblox_node/combined_occupancy_grid` (nav_msgs/OccupancyGrid) - 2D occupancy grid map
- `/nvblox_node/mesh` (nvblox_msgs/msg/Mesh)- 3D mesh representation of environment
- `/visual_slam/tracking/odometry` (nav_msgs/msg/Odometry) - Visual odometry
- `/plan` (nav_msgs/msg/Path) - Planned navigation path

## Configuration

The path planning system supports various configuration modes:

### Nvblox Modes
- **Static Mode**: For static environments
- **People Segmentation**: Handles dynamic human obstacles
- **Dynamic Mode**: For environments with moving objects

### Launch Parameters
The service can be configured via launch file parameters:
- `num_cameras`: Number of cameras to use (default: 1)
- `mode`: Nvblox operation mode (static/people_segmentation/dynamic)
- `navigation`: Enable/disable Nav2 navigation (default: true)
- `lidar`: Enable 3D lidar support (default: false)
- `people_segmentation`: Model type for people segmentation

### Example Configuration
```python
# From carter_launch.py
args.add_arg('mode', default=NvbloxMode.static)
args.add_arg('navigation', True)
args.add_arg('num_cameras', 1)
args.add_arg('lidar', False)
```

## Advanced Features

### People Segmentation
Specialized handling of dynamic human obstacles:
- **PeopleSemSegNet Vanilla**: Standard people segmentation
- **PeopleSemSegNet ShuffleSeg**: Optimized lightweight segmentation
- Real-time tracking of people in the environment
- Dynamic obstacle avoidance for moving humans

### Multi-Camera Setup
Support for multiple camera configurations:
- Single camera mode (default)
- Stereo camera pairs
- Multiple camera arrays for 360Â° coverage

### Nav2 Integration
Full ROS2 navigation stack integration:
- Global path planning with A* or other algorithms
- Local path planning with DWB or TEB planners
- Costmap generation and obstacle inflation
- Recovery behaviors for navigation failures

## Performance Optimization

### GPU Memory Management
- Nvblox requires significant GPU memory for real-time mapping
- Configure voxel resolution based on available memory
- Use mesh decimation for visualization performance

### CPU Processing
- VSLAM requires substantial CPU resources
- Consider multi-threaded processing for real-time performance
- Optimize camera frame rates based on system capabilities

### Network Bandwidth
- 3D mesh topics can consume significant bandwidth
- Configure topic compression for network deployments
- Adjust publishing rates based on network capacity

## Troubleshooting

### Common Issues

#### No Camera Topics
```bash
# Check if Isaac Sim is publishing required camera topics
ros2 topic list | grep camera
ros2 topic echo /front_stereo_camera/left/camera_info
```

#### VSLAM Initialization Issues
```bash
# Monitor visual SLAM status
ros2 topic echo /visual_slam/tracking/odometry

# Check IMU data
ros2 topic echo /front_stereo_imu/imu
```

#### Navigation Not Working
```bash
# Check if navigation goals are being received
ros2 topic echo /goal_pose

# Monitor navigation status
ros2 topic echo /cmd_vel

# Check map generation
ros2 topic echo /nvblox_node/combined_occupancy_grid --field info
```

#### GPU Memory Issues
```bash
# Monitor GPU usage during mapping
nvidia-smi

# Check Nvblox memory usage in logs
docker compose logs -f robot-path-planning | grep -i memory
```

### Debug Commands
```bash
# Check path planning container logs
docker compose logs -f robot-path-planning

# Monitor navigation performance
ros2 topic hz /cmd_vel
ros2 topic hz /visual_slam/tracking/odometry


```

## Integration Notes

### Carter System Integration
- **Goal Pose Input**: Receives navigation goals
- **Depth Integration**: Uses depth data from the Depth Generation Service for nvblox 3D reconstruction
- **Stereo Vision Processing**: Uses stereo images for nvslam robot pose estimation
- **Complete Navigation**: Provides final robot movement commands for autonomous object following
- **Workflow Position**: Final service in the Carter pipeline - produces robot movement commands

### Coordinate Frames
- **Base Frame**: `base_link` - Robot coordinate system
- **Map Frame**: `map` - Global coordinate system from SLAM
- **Odom Frame**: `odom` - Odometry coordinate system
- **Camera Frame**: `camera_link` - Camera coordinate system

## Launch Files

The system includes several specialized launch files in `docs/Perception/launch/`:

- **`carter_launch.py`**: Main Carter navigation launcher
- **`nvblox_carter.launch.py`**: Nvblox-specific configuration
- **`vslam_carter.launch.py`**: Visual SLAM configuration
- **`nvblox_carter_navigation_vslam.launch.py`**: Navigation configuration

## Advanced Configuration

### Custom Navigation Parameters
```bash
# Modify navigation parameters
ros2 param set /local_costmap/local_costmap robot_radius 0.5
```

### Mapping Parameters
```bash
# Configure Nvblox voxel size
ros2 param set /nvblox_node voxel_size 0.05
```

## Support

For issues specific to robot path planning:
1. Check NVIDIA Isaac ROS Nvblox and VSLAM documentation
2. Verify camera calibration and IMU data quality
3. Ensure sufficient GPU and CPU resources
4. Test SLAM initialization in known environments

For Carter system integration issues, refer to the main Carter README.